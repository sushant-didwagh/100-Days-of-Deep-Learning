Perfect Sushax ğŸ‘¨â€ğŸ’»ğŸ”¥
This will look clean and professional in your **Day-1-Introduction README.md**.

You can paste this directly ğŸ‘‡

---

# ğŸ§  Perceptron â€“ Introduction

## ğŸ“œ History

The **Perceptron** was introduced in 1957 by **Frank Rosenblatt**.
It was one of the first algorithms inspired by the biological neuron and is considered the foundation of modern neural networks.

In 1969, Marvin Minsky and Seymour Papert showed its limitations (like inability to solve XOR), which slowed neural network research for years. Later, multilayer networks and backpropagation revived the field.

---

## âš™ï¸ How Perceptron Works

A perceptron is a **binary linear classifier**.

It works in 3 steps:

1. Compute weighted sum
   z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + b

2. Apply activation function (Step function)

3. Output class:

   * 1 â†’ if z > 0
   * 0 â†’ otherwise

It learns using this update rule:

w = w + learning_rate Ã— (y - y_hat) Ã— x

It adjusts weights only when it makes a mistake.

---

## ğŸš€ Where Perceptrons Are Used

* Binary classification problems
* Spam detection (spam / not spam)
* Simple image classification
* Foundation for Neural Networks
* Building block of Deep Learning

It is mainly used today for understanding the basics of neural networks.

---

## âŒ Limitations

* Can only solve **linearly separable** problems
* Cannot solve XOR problem
* Only works for binary classification
* Uses simple step activation (not suitable for complex patterns)

---

## ğŸ¯ Why It Is Important

The perceptron is the starting point of:

Neural Networks â†’ Multi-layer Perceptron â†’ Deep Learning â†’ Transformers

Understanding perceptron = understanding the roots of AI.

